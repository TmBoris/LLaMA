{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T16:57:06.416736Z",
     "iopub.status.busy": "2024-10-28T16:57:06.415929Z",
     "iopub.status.idle": "2024-10-28T16:57:07.475594Z",
     "shell.execute_reply": "2024-10-28T16:57:07.474308Z",
     "shell.execute_reply.started": "2024-10-28T16:57:06.416691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "!mv LLaMA LLaMA_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-11T16:04:56.772095Z",
     "iopub.status.busy": "2024-11-11T16:04:56.770989Z",
     "iopub.status.idle": "2024-11-11T16:04:58.435443Z",
     "shell.execute_reply": "2024-11-11T16:04:58.434416Z",
     "shell.execute_reply.started": "2024-11-11T16:04:56.772053Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/hw3_llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA'...\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "!rm -rf LLaMA\n",
    "!git clone https://github.com/TmBoris/LLaMA.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T18:53:09.343910Z",
     "iopub.status.busy": "2024-11-11T18:53:09.342459Z",
     "iopub.status.idle": "2024-11-11T18:53:09.373730Z",
     "shell.execute_reply": "2024-11-11T18:53:09.372632Z",
     "shell.execute_reply.started": "2024-11-11T18:53:09.343873Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/hw3_llama\n",
      "LLaMA\n",
      "LLaMA_main\n",
      "starter.ipynb\n",
      "tinyBenchmarks.pkl\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:05:00.120293Z",
     "iopub.status.busy": "2024-11-11T16:05:00.118931Z",
     "iopub.status.idle": "2024-11-11T16:05:00.366223Z",
     "shell.execute_reply": "2024-11-11T16:05:00.364949Z",
     "shell.execute_reply.started": "2024-11-11T16:05:00.120255Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm LLaMA_main/train.py LLaMA_main/inference.py\n",
    "!rm -rf LLaMA_main/src\n",
    "!cp LLaMA/train.py LLaMA_main/train.py\n",
    "!cp LLaMA/inference.py LLaMA_main/inference.py\n",
    "!cp -r LLaMA/src LLaMA_main/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T18:53:15.070455Z",
     "iopub.status.busy": "2024-11-11T18:53:15.069439Z",
     "iopub.status.idle": "2024-11-11T18:53:15.109053Z",
     "shell.execute_reply": "2024-11-11T18:53:15.107739Z",
     "shell.execute_reply.started": "2024-11-11T18:53:15.070420Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/hw3_llama/LLaMA_main\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA_main\n",
    "# %cd LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:05:03.082613Z",
     "iopub.status.busy": "2024-11-11T16:05:03.081717Z",
     "iopub.status.idle": "2024-11-11T16:05:03.121962Z",
     "shell.execute_reply": "2024-11-11T16:05:03.120593Z",
     "shell.execute_reply.started": "2024-11-11T16:05:03.082579Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt\n",
    "# %pip install git+https://github.com/felipemaiapolo/tinyBenchmarks\n",
    "# !git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness\n",
    "# %cd lm-evaluation-harness\n",
    "# %pip install -e .\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:05:03.328224Z",
     "iopub.status.busy": "2024-11-11T16:05:03.327164Z",
     "iopub.status.idle": "2024-11-11T16:05:03.356766Z",
     "shell.execute_reply": "2024-11-11T16:05:03.355789Z",
     "shell.execute_reply.started": "2024-11-11T16:05:03.328170Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + '/home/jupyter/.local/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T17:41:55.494157Z",
     "iopub.status.busy": "2024-11-11T17:41:55.492432Z",
     "iopub.status.idle": "2024-11-11T17:41:55.716593Z",
     "shell.execute_reply": "2024-11-11T17:41:55.715506Z",
     "shell.execute_reply.started": "2024-11-11T17:41:55.494114Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"hf_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:05:12.385519Z",
     "iopub.status.busy": "2024-11-11T16:05:12.384158Z",
     "iopub.status.idle": "2024-11-11T16:05:12.832228Z",
     "shell.execute_reply": "2024-11-11T16:05:12.831098Z",
     "shell.execute_reply.started": "2024-11-11T16:05:12.385473Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"wandb_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:05:13.199620Z",
     "iopub.status.busy": "2024-11-11T16:05:13.198513Z",
     "iopub.status.idle": "2024-11-11T16:05:13.250917Z",
     "shell.execute_reply": "2024-11-11T16:05:13.249740Z",
     "shell.execute_reply.started": "2024-11-11T16:05:13.199563Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !HYDRA_FULL_ERROR=1 python3 train.py writer.run_name=big_pretrain_ds_bs3_compile dataloader.batch_size=3 lr_scheduler.max_lr=5e-5 trainer.amp=False model.use_xformers=False trainer.resume_from=checkpoint-epoch45.pth  # pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:05:13.658613Z",
     "iopub.status.busy": "2024-11-11T16:05:13.657514Z",
     "iopub.status.idle": "2024-11-11T16:05:13.676533Z",
     "shell.execute_reply": "2024-11-11T16:05:13.675318Z",
     "shell.execute_reply.started": "2024-11-11T16:05:13.658562Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !HYDRA_FULL_ERROR=1 python3 train.py writer.run_name=fine_tune_v10 dataloader.batch_size=3 lr_scheduler.max_lr=2e-5 trainer.amp=False model.use_xformers=False +trainer.from_pretrained=saved/big_pretrain_ds_bs3_compile/checkpoint-epoch100.pth trainer.n_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:49:16.334160Z",
     "iopub.status.busy": "2024-11-11T16:49:16.332731Z",
     "iopub.status.idle": "2024-11-11T16:49:16.362903Z",
     "shell.execute_reply": "2024-11-11T16:49:16.361806Z",
     "shell.execute_reply.started": "2024-11-11T16:49:16.334113Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.model.LLaMA_config import LLaMA_config\n",
    "\n",
    "llama_config = LLaMA_config()\n",
    "llama_config.register_for_auto_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:49:18.502407Z",
     "iopub.status.busy": "2024-11-11T16:49:18.500963Z",
     "iopub.status.idle": "2024-11-11T17:09:54.311550Z",
     "shell.execute_reply": "2024-11-11T17:09:54.310365Z",
     "shell.execute_reply.started": "2024-11-11T16:49:18.502347Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized\n"
     ]
    }
   ],
   "source": [
    "from src.model.LLaMA_model import LLaMA_model\n",
    "\n",
    "llama_model = LLaMA_model(llama_config)\n",
    "llama_model.register_for_auto_class(\"AutoModelForCausalLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T17:36:47.723790Z",
     "iopub.status.busy": "2024-11-11T17:36:47.722625Z",
     "iopub.status.idle": "2024-11-11T17:38:44.488152Z",
     "shell.execute_reply": "2024-11-11T17:38:44.486908Z",
     "shell.execute_reply.started": "2024-11-11T17:36:47.723754Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2689/2242226531.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_state_dict = torch.load('saved/finetune5e-4/checkpoint-epoch10.pth')[\"state_dict\"]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "saved_state_dict = torch.load('saved/finetune5e-4/checkpoint-epoch10.pth')[\"state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T17:38:44.529815Z",
     "iopub.status.busy": "2024-11-11T17:38:44.528574Z",
     "iopub.status.idle": "2024-11-11T17:38:44.549797Z",
     "shell.execute_reply": "2024-11-11T17:38:44.548738Z",
     "shell.execute_reply.started": "2024-11-11T17:38:44.529773Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict \n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in saved_state_dict.items():\n",
    "    name = k.replace(\"_orig_mod.\", \"\")  # Убираем префикс\n",
    "    new_state_dict[name] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T17:38:44.551867Z",
     "iopub.status.busy": "2024-11-11T17:38:44.551217Z",
     "iopub.status.idle": "2024-11-11T17:38:44.806629Z",
     "shell.execute_reply": "2024-11-11T17:38:44.805412Z",
     "shell.execute_reply.started": "2024-11-11T17:38:44.551822Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_model.model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T17:09:57.281911Z",
     "iopub.status.busy": "2024-11-11T17:09:57.280750Z",
     "iopub.status.idle": "2024-11-11T17:09:57.298296Z",
     "shell.execute_reply": "2024-11-11T17:09:57.296993Z",
     "shell.execute_reply.started": "2024-11-11T17:09:57.281876Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLaMA_config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"d_model\": 768,\n",
       "  \"inter_dim\": 1024,\n",
       "  \"max_seq_len\": 1024,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"n_heads\": 16,\n",
       "  \"n_layers\": 16,\n",
       "  \"n_ropes\": 3300,\n",
       "  \"rope_coef\": 0.25,\n",
       "  \"transformers_version\": \"4.46.1\",\n",
       "  \"use_xformers\": false,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T17:09:57.300648Z",
     "iopub.status.busy": "2024-11-11T17:09:57.299714Z",
     "iopub.status.idle": "2024-11-11T17:09:57.318962Z",
     "shell.execute_reply": "2024-11-11T17:09:57.317938Z",
     "shell.execute_reply.started": "2024-11-11T17:09:57.300604Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLaMA_model(\n",
       "  (model): LLaMA(\n",
       "    (embeds): Embedding(32000, 768)\n",
       "    (blocks): ModuleList(\n",
       "      (0-15): 16 x LlamaBlock(\n",
       "        (rms): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "        (attention): RoPEMaskedMultiheadAttention(\n",
       "          (heads): ModuleList(\n",
       "            (0-15): 16 x RoPEMaskedAttentionHead(\n",
       "              (w_q): Linear(in_features=768, out_features=48, bias=True)\n",
       "              (w_k): Linear(in_features=768, out_features=48, bias=True)\n",
       "              (w_v): Linear(in_features=768, out_features=48, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feedforward): Sequential(\n",
       "          (0): SwiGLU(\n",
       "            (linear1): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (linear2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (silu): SiLU()\n",
       "          )\n",
       "          (1): Linear(in_features=1024, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rms): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "    (linear): Linear(in_features=768, out_features=32000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T17:41:58.822969Z",
     "iopub.status.busy": "2024-11-11T17:41:58.821668Z",
     "iopub.status.idle": "2024-11-11T17:42:35.591005Z",
     "shell.execute_reply": "2024-11-11T17:42:35.589849Z",
     "shell.execute_reply.started": "2024-11-11T17:41:58.822934Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:   0%|          | 0.00/824M [00:00<?, ?B/s]'(MaxRetryError(\"HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/8c/a8/8ca834b86899612d3f23914b5e626cafd249f08dd9a9447827030cc97e988369/a14223695228f9f664f27c59ebe9c0c3d4d23c8b08afe7ea2ed7f05bbf2d2999?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241111T174204Z&X-Amz-Expires=86400&X-Amz-Signature=02a9442a2ccde0f8dcdb0417a090e404711270d56d8b1bb85625698cbd558b23&X-Amz-SignedHeaders=host&partNumber=1&uploadId=RRg5HtTEpETo2EEo7LOfaL0Fpqtrlw3v2E.e9w9danNvMExQ4t97z4sDkiH2y3L3nVu4ueIszKPIPfmyoLkqa4392FTJDShpGggXeqVFj5eZSYa0QK9g.pCRf1F2_h3T&x-id=UploadPart (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2426)')))\"), '(Request ID: af86cde4-81cc-47f0-a5cc-c23fa3f85049)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/8c/a8/8ca834b86899612d3f23914b5e626cafd249f08dd9a9447827030cc97e988369/a14223695228f9f664f27c59ebe9c0c3d4d23c8b08afe7ea2ed7f05bbf2d2999?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241111T174204Z&X-Amz-Expires=86400&X-Amz-Signature=02a9442a2ccde0f8dcdb0417a090e404711270d56d8b1bb85625698cbd558b23&X-Amz-SignedHeaders=host&partNumber=1&uploadId=RRg5HtTEpETo2EEo7LOfaL0Fpqtrlw3v2E.e9w9danNvMExQ4t97z4sDkiH2y3L3nVu4ueIszKPIPfmyoLkqa4392FTJDShpGggXeqVFj5eZSYa0QK9g.pCRf1F2_h3T&x-id=UploadPart\n",
      "Retrying in 1s [Retry 1/5].\n",
      "model.safetensors: 828MB [00:30, 27.5MB/s]                            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/TmBoris/custom-llama2/commit/1c9c7ff6a662394075e664c33ac8c2eff2841c80', commit_message='Upload LLaMA_model', commit_description='', oid='1c9c7ff6a662394075e664c33ac8c2eff2841c80', pr_url=None, repo_url=RepoUrl('https://huggingface.co/TmBoris/custom-llama2', endpoint='https://huggingface.co', repo_type='model', repo_id='TmBoris/custom-llama2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_model.push_to_hub(\"custom-llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T17:51:08.249226Z",
     "iopub.status.busy": "2024-11-11T17:51:08.247696Z",
     "iopub.status.idle": "2024-11-11T18:12:59.820826Z",
     "shell.execute_reply": "2024-11-11T18:12:59.819586Z",
     "shell.execute_reply.started": "2024-11-11T17:51:08.249166Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized\n",
      "hf (pretrained=TmBoris/custom-llama2,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1\n",
      "| Tasks  |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|--------|------:|------|-----:|--------|---|-----:|---|------|\n",
      "|tinyMMLU|      0|none  |     0|acc_norm|↑  |0.2925|±  |   N/A|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2024-11-11 17:51:16.057890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "2024-11-11:17:51:19,775 INFO     [__main__.py:279] Verbosity set to INFO\n",
      "2024-11-11:17:51:36,918 INFO     [__main__.py:364] Passed `--trust_remote_code`, setting environment variable `HF_DATASETS_TRUST_REMOTE_CODE=true`\n",
      "2024-11-11:17:51:36,918 INFO     [__main__.py:376] Selected Tasks: ['tinyMMLU']\n",
      "2024-11-11:17:51:36,926 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2024-11-11:17:51:36,926 INFO     [evaluator.py:201] Initializing hf model, with arguments: {'pretrained': 'TmBoris/custom-llama2', 'trust_remote_code': True}\n",
      "2024-11-11:17:51:36,983 INFO     [huggingface.py:130] Using device 'cuda:0'\n",
      "2024-11-11:17:51:37,923 INFO     [huggingface.py:483] Using model type 'default'\n",
      "2024-11-11:17:51:40,190 INFO     [huggingface.py:367] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "Using the latest cached version of the dataset since tinyBenchmarks/tinyMMLU couldn't be found on the Hugging Face Hub\n",
      "2024-11-11:18:12:30,890 WARNING  [load.py:1442] Using the latest cached version of the dataset since tinyBenchmarks/tinyMMLU couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'all' at /home/jupyter/datasphere/project/datasetscache/tinyBenchmarks___tiny_mmlu/all/0.0.0/cd6a8ee9e1e6f3f18f683535f6c096c386a85da3 (last modified on Fri Nov  8 10:58:24 2024).\n",
      "2024-11-11:18:12:30,894 WARNING  [cache.py:94] Found the latest cached dataset configuration 'all' at /home/jupyter/datasphere/project/datasetscache/tinyBenchmarks___tiny_mmlu/all/0.0.0/cd6a8ee9e1e6f3f18f683535f6c096c386a85da3 (last modified on Fri Nov  8 10:58:24 2024).\n",
      "2024-11-11:18:12:30,962 WARNING  [model.py:422] model.chat_template was called with the chat_template set to False or None. Therefore no chat template will be applied. Make sure this is an intended behavior.\n",
      "2024-11-11:18:12:30,965 INFO     [task.py:415] Building contexts for tinyMMLU on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 1837.04it/s]\n",
      "2024-11-11:18:12:31,028 INFO     [evaluator.py:489] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|██████████| 400/400 [00:22<00:00, 17.44it/s]\n",
      "2024-11-11:18:12:57,863 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
      "2024-11-11:18:12:57,868 INFO     [evaluation_tracker.py:287] Saving per-sample results for: tinyMMLU\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "            --model_args pretrained=TmBoris/custom-llama2 \\\n",
    "            --tasks tinyMMLU --batch_size 1 \\\n",
    "            --output_path saved/metrics/finetune5e_4 \\\n",
    "            --log_samples \\\n",
    "            --device cuda:0 \\\n",
    "            --trust_remote_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T18:53:42.246339Z",
     "iopub.status.busy": "2024-11-11T18:53:42.244966Z",
     "iopub.status.idle": "2024-11-11T18:53:44.582302Z",
     "shell.execute_reply": "2024-11-11T18:53:44.581004Z",
     "shell.execute_reply.started": "2024-11-11T18:53:42.246303Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mmlu': {'irt': 0.26738569790679084, 'pirt': 0.2966840485183084, 'gpirt': 0.2934334594298786}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tinyBenchmarks as tb\n",
    "\n",
    "\n",
    "file_path = \"saved/metrics/finetune5e_4/TmBoris__custom-llama/samples_tinyMMLU_2024-11-11T17-29-57.966114.jsonl\"\n",
    "\n",
    "results = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        results.append(json.loads(line))\n",
    "\n",
    "# Ensuring correct order of outputs  \n",
    "results = sorted(results, key=lambda x: x['doc_id'])\n",
    "\n",
    "y = np.array([float(item['acc_norm']) for item in results])\n",
    "\n",
    "metric_results = tb.evaluate(y, 'mmlu')\n",
    "print(metric_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-08T17:20:37.313027Z",
     "iopub.status.idle": "2024-11-08T17:20:37.313584Z",
     "shell.execute_reply": "2024-11-08T17:20:37.313333Z",
     "shell.execute_reply.started": "2024-11-08T17:20:37.313311Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
