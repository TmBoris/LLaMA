_target_: src.model.LLaMA
vocab_size: ${loss_function.vocab_size}
d_model: 768
n_heads: 16
n_ropes: 3300
rope_coef: 0.25
inter_dim: 1024
n_layers: 16
max_seq_len: 1024 # set to 1024 for ft
use_xformers: True