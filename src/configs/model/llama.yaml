_target_: src.model.LLaMA
vocab_size: ${loss_function.vocab_size}
d_model: 768
n_heads: 16
seq_len: 256 # train seq_len
expected_seq_len: 256 # set to 1024 for ft
inter_dim: 1024
n_layers: 16
use_xformers: True