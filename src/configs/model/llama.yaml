_target_: src.model.LLaMA
vocab_size: ${loss_function.vocab_size}
d_model: 768
n_heads: 16
n_ropes: 256
rope_coef: 1
inter_dim: 1024
n_layers: 16
max_seq_len: 256 # set to 1024 for ft
use_xformers: True